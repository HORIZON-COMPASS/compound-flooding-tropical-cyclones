{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare rainfall forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the correct packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "\n",
    "import hydromt\n",
    "from hydromt.log import setuplog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case study settings\n",
    "start_date = np.datetime64('2019-03-09T00:00') \n",
    "end_date = np.datetime64('2019-03-25T06:00') \n",
    "\n",
    "# Define offset for ERA5 & ERA5-Land correction\n",
    "offset = pd.tseries.frequencies.to_offset(\"1h\")\n",
    "\n",
    "# Region of case study area in Mozambique\n",
    "bbox = (34.33,-20.12,34.95,-19.30)\n",
    "lat = slice(-19.30, -20.12)\n",
    "lon = slice(34.33, 34.95)\n",
    "\n",
    "# region of eastern Africa\n",
    "bbox_big = (29,-27,46,-9)\n",
    "lat_big = slice(-9, -27)\n",
    "lon_big = slice(29, 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in ERA5 the data from wflow folder\n",
    "era5_hourly = xr.open_dataset(r\"p:\\wflow_global\\hydromt\\meteo\\era5\\tp\\era5_tp_2019_hourly.nc\", engine='netcdf4')\n",
    "# era5_hourly = xr.open_dataset(r'p:\\11210471-001-compass\\01_Data\\ERA5\\Idai\\time_corrected\\era5_tp.nc', engine='netcdf4')\n",
    "\n",
    "# select time range and spatial extent\n",
    "era5_precip_idai  = era5_hourly['tp'].sel(time=slice(start_date, end_date), latitude=lat, longitude=lon)*1000\n",
    "era5_precip_idai_big = era5_hourly['tp'].sel(time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset the time by one hour so all the correct hourly time steps in a day are selected\n",
    "# era5_hourly[\"time\"] = era5_hourly.get_index(\"time\") - offset\n",
    "# We store it for ERA5 correction\n",
    "# time_index = era5_hourly[\"time\"]\n",
    "\n",
    "# era5_precip_idai_big = era5_hourly['tp'].sel(time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)*1000\n",
    "\n",
    "# era5_final_corrected = era5_precip_idai_big.to_dataset()\n",
    "\n",
    "# # Export corrected 1h shifted ERA5, encoding otherwise wflow doesn't run\n",
    "# chunksizes = (era5_final_corrected.latitude.size, era5_final_corrected.longitude.size, 1)\n",
    "# encoding = {\n",
    "#         v: {\"zlib\": True, \"dtype\": \"float32\", \"chunksizes\": chunksizes}\n",
    "#         for v in era5_final_corrected.data_vars.keys()\n",
    "#     }\n",
    "# encoding[\"time\"] = {\"_FillValue\": None}\n",
    "\n",
    "# outfile_path = r'p:\\11210471-001-compass\\01_Data\\ERA5\\Idai\\time_corrected\\era5_tp.nc'\n",
    "# #%%We save the output\n",
    "# era5_final_corrected.to_netcdf(outfile_path, encoding=encoding)\n",
    "# era5_final_corrected.close()\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the CHIRPS data\n",
    "chirps_data = xr.open_dataset(r\"p:\\wflow_global\\hydromt\\meteo\\chirps_africa_daily_v2.0\\CHIRPS_rainfall_2019.nc\", engine='netcdf4')\n",
    "chirps_data = chirps_data.rename({ \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "# select time range\n",
    "chirps_precip_idai     = chirps_data['precipitation'].sel(time=slice(start_date, end_date), latitude=lat, longitude=lon)\n",
    "chirps_precip_idai_big = chirps_data['precipitation'].sel(time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_data.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the TAMSAT data (from https://research.reading.ac.uk/tamsat/data-access/)\n",
    "tamsat_data     = xr.open_dataset(r\"p:\\11210471-001-compass\\01_Data\\TAMSAT\\Idai\\01-tamsatDaily.v3.1-20190301-20190331-20241002_34.0_35.0_-21.0_-19.0.nc\", engine='netcdf4')\n",
    "tamsat_data_big = xr.open_dataset(r\"p:\\11210471-001-compass\\01_Data\\TAMSAT\\Idai\\Whole_MZ\\01-tamsatDaily.v3.1-20190314-20190323-20240821_29.0_46.0_-27.0_-9.0.nc\", engine='netcdf4')\n",
    "# Renaming lat and lon\n",
    "tamsat_data = tamsat_data.rename({ \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "tamsat_data_big = tamsat_data_big.rename({ \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "# select time range and filled rainfall variable (non filled is 'rfe)\n",
    "tamsat_precip_idai     = tamsat_data['rfe_filled'].sel(time=slice(start_date, end_date), latitude=lat, longitude=lon)\n",
    "tamsat_precip_idai_big = tamsat_data_big['rfe_filled'].sel(time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ERA5-Land data, corrected for time and precip units from m to mm\n",
    "era5_land = xr.open_dataset(r'p:\\11210471-001-compass\\01_Data\\ERA5_land\\units_corrected\\era5land_tp_long.nc', engine='netcdf4')\n",
    "\n",
    "# sum over time range for the study region\n",
    "era5land_precip_idai     = era5_land['tp'].sel(valid_time=slice(start_date, end_date), latitude=lat, longitude=lon)\n",
    "era5land_precip_idai_big = era5_land['tp'].sel(valid_time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5_land     = xr.open_dataset(r\"p:\\11210471-001-compass\\01_Data\\ERA5_land\\Idai_rainfall\\era5_land_20190301-20190331_MZregion.nc\", engine='netcdf4')\n",
    "# # era5_land_big = xr.open_dataset(r\"p:\\11210471-001-compass\\01_Data\\ERA5_land\\Idai_rainfall\\ERA5_land_CDSbeta.nc\", engine='netcdf4')\n",
    "# # ERA5 Land data from 9 March, so longer temporal range\n",
    "# era5_land_big = xr.open_dataset(r\"p:\\11210471-001-compass\\01_Data\\ERA5_land\\Idai_rainfall\\era5_land_MZ_long.nc\", engine='netcdf4')\n",
    "\n",
    "# # To take the correct days we go from 01:00 until 00:00 of the NEXT day, see the link # https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790\n",
    "# era5_land[\"valid_time\"] = era5_land.get_index(\"valid_time\") - offset\n",
    "# era5_land_big[\"valid_time\"] = era5_land_big.get_index(\"valid_time\") - offset\n",
    "# # ?? Or shift the time back?\n",
    "\n",
    "# # We create the function and pad with 0 so the output array has the same length\n",
    "# def diff_len(x):\n",
    "#     return np.diff(x, n=1, prepend=0)\n",
    "\n",
    "# # We use the .resample() and assign the correct time index\n",
    "# ds= era5_land.resample(valid_time=\"D\")\n",
    "# era5_land_corr = xr.apply_ufunc(\n",
    "#     # function to apply\n",
    "#     diff_len,\n",
    "#     # object with data to pass to function\n",
    "#     ds,\n",
    "#     input_core_dims=[['valid_time']],\n",
    "#     output_core_dims = [['valid_time']])\n",
    "\n",
    "# # Do the same for the ERA5-Land for the whole region of eastern Africa\n",
    "# ds= era5_land_big.resample(valid_time=\"D\")\n",
    "# era5_land_big_corr = xr.apply_ufunc(\n",
    "#     # function to apply\n",
    "#     diff_len,\n",
    "#     # object with data to pass to function\n",
    "#     ds,\n",
    "#     input_core_dims=[['valid_time']],\n",
    "#     output_core_dims = [['valid_time']])\n",
    "\n",
    "# # sum over time range for the study region and change total rainfall unit from m to mm\n",
    "# era5land_precip_idai     = era5_land_corr['tp'].sel(valid_time=slice(start_date, end_date), latitude=lat, longitude=lon)*1000\n",
    "# era5land_precip_idai_big = era5_land_big_corr['tp'].sel(valid_time=slice(start_date, end_date), latitude=lat_big, longitude=lon_big)*1000\n",
    "\n",
    "# # Save the corrected ERA5-Land to a dataset\n",
    "# era5land_final_corrected = era5land_precip_idai_big.to_dataset()\n",
    "\n",
    "# # Export corrected ERA5-Land, encoding otherwise wflow doesn't run\n",
    "# chunksizes = (era5land_final_corrected.latitude.size, era5land_final_corrected.longitude.size, 1)\n",
    "# encoding = {\n",
    "#         v: {\"zlib\": True, \"dtype\": \"float32\", \"chunksizes\": chunksizes}\n",
    "#         for v in era5land_final_corrected.data_vars.keys()\n",
    "#     }\n",
    "# encoding[\"valid_time\"] = {\"_FillValue\": None}\n",
    "\n",
    "# outfile_path = r'p:\\11210471-001-compass\\01_Data\\ERA5_land\\units_corrected\\era5land_tp_long.nc'\n",
    "# #%%We save the output\n",
    "# era5land_final_corrected.to_netcdf(outfile_path, encoding=encoding)\n",
    "# era5land_final_corrected.close()\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Mean Rainfall Across Different Datasets for Beira-Region (no time dimension correction)', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai, \"ERA5 Mean Rainfall (hourly)\"),\n",
    "    (chirps_precip_idai, \"CHIRPS Mean Rainfall (daily)\"),\n",
    "    (tamsat_precip_idai, \"TAMSAT Mean Rainfall (daily)\"),\n",
    "    (era5land_precip_idai, \"ERA5-Land Mean Rainfall (hourly)\")\n",
    "]\n",
    "\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "\n",
    "    # Calculate the spatial mean\n",
    "    mean_data = data.mean(dim=['latitude', 'longitude'])\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Sum Rainfall Across Different Datasets for Beira-Region (corrected to daily values - diff grids)', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai, \"ERA5 Sum Rainfall\"),\n",
    "    (chirps_precip_idai, \"CHIRPS Sum Rainfall\"),\n",
    "    (tamsat_precip_idai, \"TAMSAT Sum Rainfall\"),\n",
    "    (era5land_precip_idai, \"ERA5-Land Sum Rainfall\"),\n",
    "]\n",
    "\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    # Convert to daily data, to align all datsets\n",
    "    if 'time' in data.dims:\n",
    "        daily_data = data.resample(time='D').sum()\n",
    "    elif 'valid_time' in data.dims:\n",
    "        daily_data = data.resample(valid_time='D').sum()\n",
    "\n",
    "    # Calculate mean using 'latitude' and 'longitude'\n",
    "    # mean_data = daily_data.mean(dim=['latitude', 'longitude'])\n",
    "    summed_data = daily_data.sum(dim=['latitude', 'longitude'])\n",
    "\n",
    "\n",
    "    # Plot the mean data\n",
    "    summed_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n",
    "\n",
    "## Note: ERA5 & the ISIMIP datasets account for rainfall over the ocean, leading to higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots for the case study area\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Sum Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai, \"ERA5 Total Rainfall\"),\n",
    "    (chirps_precip_idai, \"CHIRPS Total Rainfall\"),\n",
    "    (tamsat_precip_idai, \"TAMSAT Total Rainfall\"),\n",
    "    (era5land_precip_idai, \"ERA5-Land Total Rainfall\")\n",
    "]\n",
    "\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        sum_data = data.sum(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        sum_data = data.sum(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    sum_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n",
    "\n",
    "## Note: ERA5 & the ISIMIP datasets account for rainfall over the ocean, leading to higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Mean Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai, \"ERA5 Mean Rainfall\"),\n",
    "    (chirps_precip_idai, \"CHIRPS Mean Rainfall\"),\n",
    "    (tamsat_precip_idai, \"TAMSAT Mean Rainfall\"),\n",
    "    (era5land_precip_idai, \"ERA5-Land Mean Rainfall\")\n",
    "]\n",
    "\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        mean_data = data.mean(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        mean_data = data.mean(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Mean Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai_big, \"ERA5 Mean Rainfall\"),\n",
    "    (chirps_precip_idai_big, \"CHIRPS Mean Rainfall\"),\n",
    "    (tamsat_precip_idai_big, \"TAMSAT Mean Rainfall\"),\n",
    "    (era5land_precip_idai_big, \"ERA5-Land Mean Rainfall\")\n",
    "    \n",
    "]\n",
    "\n",
    "# Calculate the mean of rainfall over time\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        mean_data = data.mean(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        mean_data = data.mean(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Sum Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai_big, \"ERA5 Sum Rainfall\"),\n",
    "    (chirps_precip_idai_big, \"CHIRPS Sum Rainfall\"),\n",
    "    (tamsat_precip_idai_big, \"TAMSAT Sum Rainfall\"),\n",
    "    (era5land_precip_idai_big, \"ERA5-Land Sum Rainfall\")\n",
    "]\n",
    "\n",
    "# Calculate the mean of rainfall over time, can be changed to calculate the sum\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        mean_data = data.sum(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        mean_data = data.sum(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Sum Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_precip_idai_big, \"ERA5 Sum Rainfall\"),\n",
    "    (chirps_precip_idai_big, \"CHIRPS Sum Rainfall\"),\n",
    "    (tamsat_precip_idai_big, \"TAMSAT Sum Rainfall\"),\n",
    "    (era5land_precip_idai_big, \"ERA5-Land Sum Rainfall\")\n",
    "]\n",
    "\n",
    "# Calculate the mean of rainfall over time, can be changed to calculate the sum\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        mean_data = data.sum(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        mean_data = data.sum(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using hydromt to avoid mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_cat = \"C:/Code/COMPASS/Workflows/03_data_catalogs/datacatalog_general.yml\"\n",
    "wflow_region  = \"p:/11210471-001-compass/02_Models/sofala/Idai/wflow/staticgeoms/region.geojson\"\n",
    "wflow_basins  = \"p:/11210471-001-compass/02_Models/sofala/Idai/wflow/staticgeoms/basins.geojson\"\n",
    "\n",
    "region = gpd.read_file(wflow_region)\n",
    "basins = gpd.read_file(wflow_basins)\n",
    "\n",
    "# Read data catalog\n",
    "data_catalog = hydromt.data_catalog.DataCatalog(data_libs = path_data_cat)\n",
    "\n",
    "# Pass as a time tuple for HydroMT\n",
    "time_range = (start_date, end_date)\n",
    "\n",
    "# Load raster data for specified region and time range\n",
    "era5_tp = data_catalog.get_rasterdataset(\n",
    "    data_like = 'era5_hourly_zarr',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    geom=region,\n",
    "    buffer=2\n",
    ")\n",
    "\n",
    "era5_tp_MZ = data_catalog.get_rasterdataset(\n",
    "    data_like = 'era5_hourly_zarr',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    bbox=bbox_big,\n",
    "    buffer=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5_land_tp = data_catalog.get_rasterdataset(\n",
    "#     data_like = 'ERA5land_Idai_CF0',\n",
    "#     time_tuple = time_range,\n",
    "#     variables = 'precip',\n",
    "#     geom=region\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_tp = data_catalog.get_rasterdataset(\n",
    "    data_like = 'chirps',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    geom=region,\n",
    "    buffer=2\n",
    ")\n",
    "\n",
    "chirps_tp_MZ = data_catalog.get_rasterdataset(\n",
    "    data_like = 'chirps',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    bbox=bbox_big,\n",
    "    buffer=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamsat_tp = data_catalog.get_rasterdataset(\n",
    "    data_like = 'TAMSAT_MZ',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    geom=region,\n",
    "    buffer=2\n",
    ")\n",
    "\n",
    "tamsat_tp_MZ = data_catalog.get_rasterdataset(\n",
    "    data_like = 'TAMSAT_MZ',\n",
    "    time_tuple = time_range,\n",
    "    variables = 'precip',\n",
    "    bbox=bbox_big,\n",
    "    buffer=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import PowerNorm\n",
    "from shapely.geometry import MultiLineString\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "# fig.suptitle('Comparison of Sum Rainfall Across Different Datasets', fontsize=16)\n",
    "\n",
    "# List of datasets and titles\n",
    "datasets = [\n",
    "    (era5_tp_MZ, \"(a) ERA5\"),\n",
    "    (chirps_tp_MZ, \"(b) CHIRPS\"),\n",
    "    (tamsat_tp_MZ, \"(c) TAMSAT\"),\n",
    "]\n",
    "\n",
    "# Sum and determine color limits\n",
    "accums = []\n",
    "for data, _ in datasets:\n",
    "    time_dim = 'time' if 'time' in data.dims else 'valid_time'\n",
    "    accums.append(data.sum(dim=time_dim))\n",
    "vmin = min([a.min().compute().item() for a in accums])\n",
    "vmax = max([a.max().compute().item() for a in accums])\n",
    "norm = PowerNorm(gamma=0.5, vmin=vmin, vmax=vmax)\n",
    "cmap = cm.get_cmap(\"viridis\")\n",
    "\n",
    "# Convert and extract boundary\n",
    "basins_latlon = basins.to_crs(epsg=4326)\n",
    "merged_geom = basins_latlon.unary_union\n",
    "if merged_geom.geom_type == 'Polygon':\n",
    "    outer_boundary = merged_geom.exterior\n",
    "elif merged_geom.geom_type == 'MultiPolygon':\n",
    "    outer_boundary = MultiLineString([poly.exterior for poly in merged_geom.geoms])\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected geometry type: {merged_geom.geom_type}\")\n",
    "\n",
    "# Get extent\n",
    "minx, miny, maxx, maxy = region.total_bounds\n",
    "minx_big, miny_big, maxx_big, maxy_big = bbox_big\n",
    "\n",
    "# Loop through both rows: top row = large region, bottom = zoomed in\n",
    "for i, ((data, title), summed) in enumerate(zip(datasets, accums)):\n",
    "    for row in [0, 1]:  # 0 = large, 1 = zoom\n",
    "        ax = axes[row, i]\n",
    "        im = summed.plot(\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "            add_colorbar=False\n",
    "        )\n",
    "        \n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.6) \n",
    "        ax.add_geometries([outer_boundary], crs=ccrs.PlateCarree(),\n",
    "                          edgecolor='white', facecolor='none', linewidth=0.5)\n",
    "        if row == 0:\n",
    "            # Full extent (wider region)\n",
    "            ax.set_extent([minx_big, maxx_big-1.2, miny_big, maxy_big], crs=ccrs.PlateCarree())\n",
    "            ax.set_title(f\"{title}\")\n",
    "        else:\n",
    "            ax.set_extent([minx - 0.05, maxx + 0.05, miny - 0.05, maxy + 0.05], crs=ccrs.PlateCarree())\n",
    "            ax.set_title(f\"\")\n",
    "\n",
    "        # Add gridlines with lat/lon labels\n",
    "        gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.top_labels = False         # You can set this to True if needed\n",
    "        gl.right_labels = False       # Typically False unless you want labels on the rightmost plots\n",
    "        gl.bottom_labels = (row == 1) # Only bottom row gets bottom labels\n",
    "        gl.left_labels = (i == 0)     # Only leftmost plots get y-axis labels\n",
    "        gl.xlabel_style = {'size': 10}\n",
    "        gl.ylabel_style = {'size': 10}\n",
    "\n",
    "# Add colorbar below all subplots\n",
    "cbar_ax = fig.add_axes([0.36, 0.035, 0.3, 0.02])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label(\"Accumulated precipitation (mm)\")\n",
    "\n",
    "# Adjust spacing between rows\n",
    "plt.subplots_adjust(hspace=0.03, wspace=0.03)\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0.07, 1, 1],  w_pad=0.2)  # Leave space at bottom for colorbar\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle('Comparison of Sum Rainfall Across Different Datasets', fontsize=16, y=1.02)\n",
    "\n",
    "# Plot each dataset in separate subplots\n",
    "datasets = [\n",
    "    (era5_tp, \"ERA5 Sum Rainfall\"),\n",
    "    (chirps_precip_idai_big, \"CHIRPS Sum Rainfall\"),\n",
    "    (tamsat_precip_idai_big, \"TAMSAT Sum Rainfall\"),\n",
    "    (era5land_precip_idai_big, \"ERA5-Land Sum Rainfall\")\n",
    "]\n",
    "\n",
    "# Calculate the mean of rainfall over time, can be changed to calculate the sum\n",
    "for ax, (data, title) in zip(axes.flatten(), datasets):\n",
    "    if 'time' in data.dims:\n",
    "        mean_data = data.sum(dim='time')\n",
    "    elif 'valid_time' in data.dims:\n",
    "        mean_data = data.sum(dim='valid_time')\n",
    "\n",
    "    # Plot the mean data\n",
    "    mean_data.plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patheffects as pe\n",
    "from cartopy.io.shapereader import Reader, natural_earth\n",
    "import netCDF4 as nc\n",
    "# from ibtracs import Ibtracs\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import traceback\n",
    "\n",
    "class StormFromCSV:\n",
    "    \"\"\"\n",
    "    A simple class to mimic the IBTrACS storm object when loading from CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, season, lats, lons, times, winds, basin=\"SI\"):\n",
    "        self.name = str(name)\n",
    "        self.season = int(season)\n",
    "        self.lats = np.array(lats, dtype=float)\n",
    "        self.lons = np.array(lons, dtype=float)\n",
    "        self.times = (\n",
    "            pd.to_datetime(times) if not isinstance(times, pd.DatetimeIndex) else times\n",
    "        )\n",
    "        # Handle wind data more carefully - preserve NaN values\n",
    "        if isinstance(winds, (list, tuple, np.ndarray)):\n",
    "            winds_array = np.array(\n",
    "                winds, dtype=float\n",
    "            )  # This will convert to float, keeping NaN\n",
    "            self.wind = winds_array\n",
    "        else:\n",
    "            self.wind = np.array(\n",
    "                [float(winds) if not np.isnan(float(winds)) else np.nan]\n",
    "            )\n",
    "        self.basin = str(basin)\n",
    "        # Set genesis to first time point if available\n",
    "        if len(self.times) > 0:\n",
    "            self.genesis = (\n",
    "                self.times.iloc[0] if hasattr(self.times, \"iloc\") else self.times[0]\n",
    "            )\n",
    "        else:\n",
    "            self.genesis = None\n",
    "\n",
    "\n",
    "def load_cyclone_from_csv(csv_file_path, name, year, filter_dates=None):\n",
    "    \"\"\"\n",
    "    Load cyclone data from CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file_path : str\n",
    "        Path to the IBTrACS CSV file\n",
    "    name : str\n",
    "        Storm name\n",
    "    year : int\n",
    "        Year/season of the storm\n",
    "    filter_dates : str or tuple/list of str, optional\n",
    "        Date or date range to filter the cyclone track data.\n",
    "        - If a single string (e.g., \"YYYY-MM-DD\"): Show track for this specific day.\n",
    "        - If a tuple/list of two strings (e.g., [\"YYYY-MM-DD\", \"YYYY-MM-DD\"]): Show track for this date range (inclusive).\n",
    "        Default is None (no date filtering).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    StormFromCSV object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV file with better dtype handling\n",
    "        df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "\n",
    "        # Convert SEASON column to numeric to handle string years like '2023'\n",
    "        df[\"SEASON\"] = pd.to_numeric(df[\"SEASON\"], errors=\"coerce\")\n",
    "\n",
    "        # Filter for the specific storm\n",
    "        storm_data = df[(df[\"NAME\"] == name.upper()) & (df[\"SEASON\"] == year)].copy()\n",
    "\n",
    "        if storm_data.empty:\n",
    "            print(\"No data found for {} in {}\".format(name, year))\n",
    "            return None\n",
    "\n",
    "        # Ensure LAT and LON are numeric (float), coercing errors to NaN\n",
    "        # This conversion should happen before dropna\n",
    "        for col_name in [\"LAT\", \"LON\"]:\n",
    "            if col_name in storm_data.columns:\n",
    "                storm_data[col_name] = pd.to_numeric(\n",
    "                    storm_data[col_name], errors=\"coerce\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: Column {} not found for {} ({}) before type conversion.\".format(\n",
    "                        col_name, name, year\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Clean and process the data\n",
    "        storm_data = storm_data.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "        if storm_data.empty:\n",
    "            print(\"No valid coordinate data for {} ({})\".format(name, year))\n",
    "            return None\n",
    "\n",
    "        # Sort by time\n",
    "        storm_data[\"datetime\"] = pd.to_datetime(storm_data[\"ISO_TIME\"])\n",
    "        storm_data = storm_data.sort_values(\"datetime\")\n",
    "\n",
    "        # Keep a copy of original data before filtering\n",
    "        storm_data_orig = storm_data.copy()\n",
    "\n",
    "        # Extract the data\n",
    "        lats = storm_data[\"LAT\"].values\n",
    "        lons = storm_data[\"LON\"].values\n",
    "        times = storm_data[\"datetime\"]\n",
    "\n",
    "        # Apply date filtering if requested\n",
    "        if filter_dates and len(times) > 0:\n",
    "            try:\n",
    "                mask = np.full(len(times), False)  # Default to no points\n",
    "                if isinstance(filter_dates, str):  # Single date\n",
    "                    try:\n",
    "                        target_date = pd.to_datetime(filter_dates).normalize()\n",
    "                        mask = (times >= target_date) & (\n",
    "                            times < target_date + pd.Timedelta(days=1)\n",
    "                        )\n",
    "                    except ValueError:\n",
    "                        print(\n",
    "                            \"Warning: Invalid single date format for filter_dates: {}. Skipping filtering for this storm.\".format(\n",
    "                                filter_dates\n",
    "                            )\n",
    "                        )\n",
    "                        mask = np.full(len(times), True)\n",
    "                elif (\n",
    "                    isinstance(filter_dates, (list, tuple)) and len(filter_dates) == 2\n",
    "                ):  # Date range\n",
    "                    try:\n",
    "                        start_date = pd.to_datetime(filter_dates[0]).normalize()\n",
    "                        # Inclusive end: set to end of the day\n",
    "                        end_date = (\n",
    "                            pd.to_datetime(filter_dates[1]).normalize()\n",
    "                            + pd.Timedelta(days=1)\n",
    "                            - pd.Timedelta(microseconds=1)\n",
    "                        )\n",
    "                        mask = (times >= start_date) & (times <= end_date)\n",
    "                    except ValueError:\n",
    "                        print(\n",
    "                            \"Warning: Invalid date range format for filter_dates: {}. Skipping filtering for this storm.\".format(\n",
    "                                filter_dates\n",
    "                            )\n",
    "                        )\n",
    "                        mask = np.full(len(times), True)\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Warning: Invalid filter_dates format: {}. Skipping date filtering for this storm.\".format(\n",
    "                            filter_dates\n",
    "                        )\n",
    "                    )\n",
    "                    mask = np.full(len(times), True)\n",
    "\n",
    "                # Apply the mask to filter data\n",
    "                lats = lats[mask]\n",
    "                lons = lons[mask]\n",
    "                times = times[mask]\n",
    "                storm_data = storm_data[\n",
    "                    mask\n",
    "                ]  # Also filter the storm_data for wind processing\n",
    "\n",
    "                if not len(lats):\n",
    "                    print(\n",
    "                        \"Warning: No data points for {} ({}) within the specified date filter: {}. Using full track.\".format(\n",
    "                            name, year, filter_dates\n",
    "                        )\n",
    "                    )\n",
    "                    # Reload original data if filtering resulted in empty dataset\n",
    "                    lats = storm_data_orig[\"LAT\"].values\n",
    "                    lons = storm_data_orig[\"LON\"].values\n",
    "                    times = storm_data_orig[\"datetime\"]\n",
    "                    storm_data = storm_data_orig\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Applied date filter: {} -> {} track points\".format(\n",
    "                            filter_dates, len(lats)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            except Exception as date_filter_error:\n",
    "                print(\"Error in date filtering: {}\".format(str(date_filter_error)))\n",
    "                # Keep original data if filtering fails\n",
    "\n",
    "        # Use USA_WIND first (as it worked for Freddy), then try other wind columns\n",
    "        wind_column = None\n",
    "        for col in [\"USA_WIND\", \"WMO_WIND\", \"REUNION_WIND\", \"BOM_WIND\"]:\n",
    "            if col in storm_data.columns and not storm_data[col].isna().all():\n",
    "                wind_column = col\n",
    "                break\n",
    "\n",
    "        if wind_column:\n",
    "            # Convert wind data to numeric (handles string values like \" 35\" or \"45.0\")\n",
    "            winds_series = pd.to_numeric(storm_data[wind_column], errors=\"coerce\")\n",
    "\n",
    "            # Instead of filling all NaN with 0, only fill NaN where we have valid position data\n",
    "            # This preserves the original wind structure better\n",
    "            winds = winds_series.values\n",
    "\n",
    "            print(\"Using wind data from column: {}\".format(wind_column))\n",
    "\n",
    "            # Debug: Show wind data statistics\n",
    "            valid_winds = winds[~np.isnan(winds)]\n",
    "            if len(valid_winds) > 0:\n",
    "                print(\n",
    "                    \"Wind data stats - Count: {}, Min: {:.1f}, Max: {:.1f}, Mean: {:.1f} knots\".format(\n",
    "                        len(valid_winds),\n",
    "                        valid_winds.min(),\n",
    "                        valid_winds.max(),\n",
    "                        valid_winds.mean(),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Warning: No valid wind data found in column {}\".format(wind_column)\n",
    "                )\n",
    "        else:\n",
    "            print(\"No wind data available for {} ({}), using zeros\".format(name, year))\n",
    "            winds = np.zeros(len(lats))  # Default to zeros if no wind data\n",
    "\n",
    "        # Determine basin from the data if available\n",
    "        basin = storm_data[\"BASIN\"].iloc[0] if \"BASIN\" in storm_data.columns else \"SI\"\n",
    "\n",
    "        print(\n",
    "            \"Successfully loaded {} track points for {} ({}) from CSV\".format(\n",
    "                len(lats), name, year\n",
    "            )\n",
    "        )\n",
    "        print(\"Date range: {} to {}\".format(times.min(), times.max()))\n",
    "        # Only print wind range if we have valid wind data\n",
    "        valid_winds = winds[~np.isnan(winds) & (winds > 0)]\n",
    "        if len(valid_winds) > 0:\n",
    "            print(\n",
    "                \"Wind range: {:.1f} to {:.1f} knots\".format(\n",
    "                    valid_winds.min(), valid_winds.max()\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"No valid wind data found\")\n",
    "\n",
    "        # Create and return the storm object\n",
    "        storm_obj = StormFromCSV(name.upper(), year, lats, lons, times, winds, basin)\n",
    "\n",
    "        # Verify the object was created correctly\n",
    "        print(\n",
    "            \"Storm object created: {}, {} points\".format(\n",
    "                storm_obj.name, len(storm_obj.lats)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return storm_obj\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error loading {} ({}) from CSV: {}: {}\".format(\n",
    "                name, year, type(e).__name__, str(e)\n",
    "            )\n",
    "        )\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def plot_cyclone_precipitation(\n",
    "    nc_file,\n",
    "    bbox=None,\n",
    "    cyclone_list=None,\n",
    "    highlight_countries=None,\n",
    "    highlight_colors=None,\n",
    "    cyclone_colors=None,\n",
    "    title=None,\n",
    "    output_file=None,\n",
    "    figsize=(14, 8),  # Adjusted for more typical figure ratio\n",
    "    dpi=300,\n",
    "    country_label_size=10,\n",
    "    min_precipitation=5.0,  # Minimum precipitation to show (mm)\n",
    "    max_precipitation=None,  # Maximum precipitation for color scaling (mm)\n",
    "    include_track=True,  # Option to include cyclone track\n",
    "    adjust_bbox=True,\n",
    "    margin_percent=0.1,\n",
    "    data_source=\"api\",  # NEW: \"api\" or \"csv\"\n",
    "    csv_file_path=None,  # NEW: Path to CSV file when data_source=\"csv\"\n",
    "    filter_dates=None,  # NEW: Date filtering for track data\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a plot showing accumulated precipitation during a tropical cyclone's lifespan.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nc_file : str\n",
    "        Path to the NetCDF file containing precipitation data\n",
    "    bbox : tuple, optional\n",
    "        Bounding box (min_lat, max_lat, min_lon, max_lon)\n",
    "    cyclone_list : list, optional\n",
    "        List of cyclones to plot tracks for (if include_track=True)\n",
    "    highlight_countries : list, optional\n",
    "        List of countries to highlight\n",
    "    highlight_colors : list, optional\n",
    "        List of colors for highlighted countries\n",
    "    cyclone_colors : list, optional\n",
    "        List of colors for cyclone tracks\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    output_file : str, optional\n",
    "        Path to save the output file\n",
    "    figsize : tuple, optional\n",
    "        Figure size (width, height)\n",
    "    dpi : int, optional\n",
    "        DPI for the saved figure\n",
    "    country_label_size : int, optional\n",
    "        Font size for country labels (0 to hide)\n",
    "    min_precipitation : float, optional\n",
    "        Minimum precipitation to show in mm\n",
    "    max_precipitation : float, optional\n",
    "        Maximum precipitation for color scaling in mm\n",
    "    include_track : bool, optional\n",
    "        Whether to include the cyclone track\n",
    "    adjust_bbox : bool, optional\n",
    "        Automatically adjust the bounding box\n",
    "    margin_percent : float, optional\n",
    "        Margin to add around the bounding box\n",
    "    data_source : str, optional\n",
    "        Data source to use: \"api\" (default) or \"csv\"\n",
    "    csv_file_path : str, optional\n",
    "        Path to the IBTrACS CSV file when data_source=\"csv\"\n",
    "    filter_dates : str or tuple/list of str, optional\n",
    "        Date or date range to filter the cyclone track data.\n",
    "        - If a single string (e.g., \"YYYY-MM-DD\"): Show track for this specific day.\n",
    "        - If a tuple/list of two strings (e.g., [\"YYYY-MM-DD\", \"YYYY-MM-DD\"]): Show track for this date range (inclusive).\n",
    "        Default is None (no date filtering).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The matplotlib figure object\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The matplotlib axes object\n",
    "    \"\"\"\n",
    "    # Load the NetCDF file\n",
    "    try:\n",
    "        ds = nc.Dataset(nc_file, \"r\")\n",
    "        print(f\"Successfully opened NetCDF file: {nc_file}\")\n",
    "\n",
    "        # Check if the required variables exist\n",
    "        if \"tp\" not in ds.variables:\n",
    "            # Look for similar variables\n",
    "            precip_vars = [\n",
    "                var for var in ds.variables if \"tp\" in var or \"precip\" in var.lower()\n",
    "            ]\n",
    "            if precip_vars:\n",
    "                precip_var_name = precip_vars[0]\n",
    "                print(f\"Using '{precip_var_name}' as precipitation variable\")\n",
    "            else:\n",
    "                print(\"Error: No precipitation variable found in the file\")\n",
    "                return None, None\n",
    "        else:\n",
    "            precip_var_name = \"tp\"\n",
    "\n",
    "        # Get the precipitation data\n",
    "        precip_data = ds.variables[precip_var_name][:]\n",
    "\n",
    "        # Get coordinate variables\n",
    "        if \"latitude\" in ds.variables:\n",
    "            lat_var = \"latitude\"\n",
    "        elif \"lat\" in ds.variables:\n",
    "            lat_var = \"lat\"\n",
    "        else:\n",
    "            print(\"No latitude variable found\")\n",
    "            return None, None\n",
    "\n",
    "        if \"longitude\" in ds.variables:\n",
    "            lon_var = \"longitude\"\n",
    "        elif \"lon\" in ds.variables:\n",
    "            lon_var = \"lon\"\n",
    "        else:\n",
    "            print(\"No longitude variable found\")\n",
    "            return None, None\n",
    "\n",
    "        lats = ds.variables[lat_var][:]\n",
    "        lons = ds.variables[lon_var][:]\n",
    "\n",
    "        time_period_str = None\n",
    "        # Common names for time variables in NetCDF files\n",
    "        possible_time_vars = [\n",
    "            \"time\",\n",
    "            \"t\",\n",
    "            \"datetime\",\n",
    "            \"date\",\n",
    "            \"forecast_time0\",\n",
    "            \"valid_time\",\n",
    "        ]\n",
    "        actual_time_var_name = None\n",
    "\n",
    "        for var_name in possible_time_vars:\n",
    "            if var_name in ds.variables:\n",
    "                actual_time_var_name = var_name\n",
    "                break\n",
    "\n",
    "        if actual_time_var_name:\n",
    "            try:\n",
    "                time_var = ds.variables[actual_time_var_name]\n",
    "                time_values = time_var[:]  # Read all time values\n",
    "                \n",
    "                if len(time_values) > 0:\n",
    "                    time_units = time_var.units\n",
    "                    # Use getattr for calendar to provide a default if it's missing\n",
    "                    time_calendar = getattr(time_var, \"calendar\", \"standard\")\n",
    "\n",
    "                    # Convert numeric time values to datetime objects\n",
    "                    first_time_dt = nc.num2date(\n",
    "                        time_values[0], units=time_units, calendar=time_calendar\n",
    "                    )\n",
    "                    last_time_dt = nc.num2date(\n",
    "                        time_values[-1], units=time_units, calendar=time_calendar\n",
    "                    )\n",
    "\n",
    "                    # Determine date format: include HH:MM unless both start/end are at midnight\n",
    "                    if (\n",
    "                        first_time_dt.hour == 0\n",
    "                        and first_time_dt.minute == 0\n",
    "                        and first_time_dt.second == 0\n",
    "                        and last_time_dt.hour == 0\n",
    "                        and last_time_dt.minute == 0\n",
    "                        and last_time_dt.second == 0\n",
    "                    ):\n",
    "                        date_format = \"%Y-%m-%d\"\n",
    "                    else:\n",
    "                        date_format = \"%Y-%m-%d %H:%M\"\n",
    "\n",
    "                    first_time_str = first_time_dt.strftime(date_format)\n",
    "                    last_time_str = last_time_dt.strftime(date_format)\n",
    "                    time_period_str = f\"{first_time_str} - {last_time_str}\"\n",
    "                    \n",
    "                    print(\n",
    "                        f\"Time period: {time_period_str} (Units: {time_units}, Calendar: {time_calendar})\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not process time variable '{actual_time_var_name}' for colorbar label: {e}\"\n",
    "                )\n",
    "                time_period_str = None  # Ensure reset on error\n",
    "        else:\n",
    "            print(\n",
    "                \"Warning: No common time variable found in NetCDF for colorbar label.\"\n",
    "            )\n",
    "\n",
    "        # Calculate accumulated precipitation (sum along time axis)\n",
    "        accumulated_precip = np.sum(precip_data, axis=0)\n",
    "\n",
    "        # Check if the units need conversion (e.g., from m to mm)\n",
    "        if hasattr(ds.variables[precip_var_name], \"units\"):\n",
    "            precip_units = ds.variables[precip_var_name].units.lower()\n",
    "            if \"m\" in precip_units and \"mm\" not in precip_units:\n",
    "                # Convert from meters to mm\n",
    "                accumulated_precip *= 1000.0\n",
    "                print(\"Converting precipitation from meters to mm\")\n",
    "\n",
    "        print(f\"Accumulated precipitation shape: {accumulated_precip.shape}\")\n",
    "        print(\n",
    "            f\"Accumulated precipitation range: {np.min(accumulated_precip)} to {np.max(accumulated_precip)} mm\"\n",
    "        )\n",
    "\n",
    "        # Create figure and axis\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "        # Extract bounding box from data if not provided\n",
    "        if bbox is None:\n",
    "            min_lat, max_lat = min(lats), max(lats)\n",
    "            min_lon, max_lon = min(lons), max(lons)\n",
    "            bbox = (min_lat, max_lat, min_lon, max_lon)\n",
    "        else:\n",
    "            min_lat, max_lat, min_lon, max_lon = bbox\n",
    "\n",
    "        ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "\n",
    "        # Create a clean map with black lines\n",
    "        ax.add_feature(cfeature.LAND, facecolor=\"#F5F5F5\", alpha=1.0)  # Light gray\n",
    "        ax.add_feature(cfeature.OCEAN, facecolor=\"white\", alpha=1.0)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor=\"black\", alpha=0.5)\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.7, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "        # Highlight specific countries if requested (with very subtle highlighting)\n",
    "        if highlight_countries and highlight_colors:\n",
    "            shp_path = natural_earth(\n",
    "                resolution=\"10m\", category=\"cultural\", name=\"admin_0_countries\"\n",
    "            )\n",
    "            reader = Reader(shp_path)\n",
    "            countries = list(reader.records())\n",
    "            central_lat = (min_lat + max_lat) / 2\n",
    "            for i, country_name in enumerate(highlight_countries):\n",
    "                color = (\n",
    "                    highlight_colors[i % len(highlight_colors)]\n",
    "                    if i < len(highlight_colors)\n",
    "                    else \"blue\"\n",
    "                )\n",
    "                for country in countries:\n",
    "                    if country.attributes[\"NAME\"].lower() == country_name.lower():\n",
    "                        # Very subtle highlighting\n",
    "                        ax.add_geometries(\n",
    "                            [country.geometry],\n",
    "                            crs=ccrs.PlateCarree(),\n",
    "                            facecolor=color,\n",
    "                            alpha=0.05,  # Very transparent\n",
    "                            edgecolor=\"black\",\n",
    "                            linewidth=0.7,\n",
    "                        )\n",
    "                        # Add country label if desired\n",
    "                        if country_label_size > 0:\n",
    "                            try:\n",
    "                                from functools import partial\n",
    "                                import pyproj\n",
    "                                import shapely.ops as ops\n",
    "                                from shapely.geometry import Point\n",
    "\n",
    "                                def get_country_centroid(geom, latitude_central=None):\n",
    "                                    if latitude_central is not None:\n",
    "                                        proj = pyproj.Proj(\n",
    "                                            proj=\"aeqd\", lat_0=latitude_central, lon_0=0\n",
    "                                        )\n",
    "                                        project = partial(\n",
    "                                            pyproj.transform,\n",
    "                                            pyproj.Proj(init=\"epsg:4326\"),\n",
    "                                            proj,\n",
    "                                        )\n",
    "                                        projected_geom = ops.transform(project, geom)\n",
    "                                        centroid = projected_geom.centroid\n",
    "                                        project_back = partial(\n",
    "                                            pyproj.transform,\n",
    "                                            proj,\n",
    "                                            pyproj.Proj(init=\"epsg:4326\"),\n",
    "                                        )\n",
    "                                        lon, lat = ops.transform(\n",
    "                                            project_back, Point(centroid.x, centroid.y)\n",
    "                                        ).coords[0]\n",
    "                                        return lon, lat\n",
    "                                    else:\n",
    "                                        centroid = geom.centroid\n",
    "                                        return centroid.x, centroid.y\n",
    "\n",
    "                                lon, lat = get_country_centroid(\n",
    "                                    country.geometry, central_lat\n",
    "                                )\n",
    "                                # Black text with white outline for better visibility\n",
    "                                ax.text(\n",
    "                                    lon,\n",
    "                                    lat,\n",
    "                                    country_name,\n",
    "                                    color=\"black\",\n",
    "                                    fontsize=country_label_size,\n",
    "                                    fontweight=\"bold\",\n",
    "                                    ha=\"center\",\n",
    "                                    va=\"center\",\n",
    "                                    path_effects=[\n",
    "                                        pe.withStroke(linewidth=2, foreground=\"white\")\n",
    "                                    ],\n",
    "                                    transform=ccrs.PlateCarree(),\n",
    "                                    zorder=100,\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error placing label for {country_name}: {e}\")\n",
    "                        break\n",
    "\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "        # Set transparency for lower values\n",
    "        alphas = np.linspace(\n",
    "            0, 1, 256\n",
    "        )  # Alpha values from 0 (transparent) to 1 (opaque)\n",
    "        alphas = np.power(\n",
    "            alphas, 0.5\n",
    "        )  # Apply power function to make low values more transparent\n",
    "        cmap._init()  # Initialize the colormap\n",
    "        cmap._lut[:-3, -1] = (\n",
    "            alphas  # Apply the alpha values (excluding the special colors at the end)\n",
    "        )\n",
    "\n",
    "        # Determine max precipitation for colormap scaling\n",
    "        if max_precipitation is None:\n",
    "            max_precipitation = np.max(accumulated_precip)\n",
    "            # Round up to a nice value\n",
    "            max_precipitation = np.ceil(max_precipitation / 100) * 100\n",
    "\n",
    "        # Create a normalized colormap from min_precipitation to max_precipitation\n",
    "        norm = plt.Normalize(min_precipitation, max_precipitation)\n",
    "\n",
    "        # Plot precipitation using pcolormesh\n",
    "        # This will show values below min_precipitation as transparent\n",
    "        masked_precip = np.ma.masked_less(accumulated_precip, min_precipitation)\n",
    "        precip_plot = ax.pcolormesh(\n",
    "            lons,\n",
    "            lats,\n",
    "            masked_precip,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "            zorder=10,  # Above base map features\n",
    "        )\n",
    "\n",
    "        # Add track if requested\n",
    "        if include_track and cyclone_list:\n",
    "            add_cyclone_tracks_precip(\n",
    "                ax,\n",
    "                cyclone_list,\n",
    "                cyclone_colors,\n",
    "                data_source,\n",
    "                csv_file_path,\n",
    "                filter_dates,\n",
    "                bbox,\n",
    "            )\n",
    "\n",
    "        # Add title with panel label like in reference image\n",
    "        if title:\n",
    "            # Extract TC name from the first cyclone in the list if available\n",
    "            tc_name = None\n",
    "            if cyclone_list and len(cyclone_list) > 0:\n",
    "                if isinstance(cyclone_list[0], tuple) and len(cyclone_list[0]) >= 1:\n",
    "                    tc_name = cyclone_list[0][0]\n",
    "                elif isinstance(cyclone_list[0], str):\n",
    "                    tc_name = cyclone_list[0]\n",
    "\n",
    "            # Format similar to reference image\n",
    "            if tc_name:\n",
    "                ax.text(\n",
    "                    0.03,\n",
    "                    0.95,\n",
    "                    f\"a) TC {tc_name}\",\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                    va=\"top\",\n",
    "                )\n",
    "            else:\n",
    "                ax.set_title(title, fontsize=14, loc=\"left\", pad=10)\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cbar_ax = divider.append_axes(\"right\", size=\"5%\", pad=0.1, axes_class=plt.Axes)\n",
    "        fig.add_axes(cbar_ax)  # Add the new axes to the figure\n",
    "        cbar = plt.colorbar(\n",
    "            precip_plot,\n",
    "            cax=cbar_ax,\n",
    "            orientation=\"vertical\",\n",
    "            extend=\"max\",\n",
    "        )\n",
    "        if time_period_str:\n",
    "            cbar_label = f\"Accumulated rainfall over\\n{time_period_str}\\n(mm)\"\n",
    "        else:\n",
    "            # Fallback label if time information couldn't be extracted\n",
    "            cbar_label = \"Accumulated rainfall (mm)\\n(Time period N/A)\"\n",
    "        cbar.set_label(cbar_label, fontsize=10, labelpad=10)\n",
    "\n",
    "        # Save figure if output file is provided\n",
    "        if output_file:\n",
    "            plt.savefig(output_file, dpi=dpi, bbox_inches=\"tight\")\n",
    "            print(f\"Saved figure to {output_file}\")\n",
    "\n",
    "        # Close the NetCDF file\n",
    "        ds.close()\n",
    "\n",
    "        return fig, ax\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or processing NetCDF file: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def add_cyclone_tracks_precip(\n",
    "    ax,\n",
    "    cyclone_list,\n",
    "    cyclone_colors,\n",
    "    data_source=\"api\",\n",
    "    csv_file_path=None,\n",
    "    filter_dates=None,\n",
    "    bbox=None,\n",
    "):\n",
    "    \"\"\"Add cyclone tracks to the precipitation plot.\"\"\"\n",
    "    if not cyclone_list:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Initialize data loading based on source\n",
    "        if data_source == \"api\":\n",
    "            ibt = Ibtracs()\n",
    "            ibt.load_all_storms()\n",
    "        elif data_source == \"csv\":\n",
    "            if not csv_file_path:\n",
    "                print(\"Warning: csv_file_path must be provided when data_source='csv'\")\n",
    "                return\n",
    "            ibt = None  # Not needed for CSV loading\n",
    "        else:\n",
    "            print(\"Warning: data_source must be 'api' or 'csv'\")\n",
    "            return\n",
    "\n",
    "        for i, cyclone_info in enumerate(cyclone_list):\n",
    "            if cyclone_colors and i < len(cyclone_colors):\n",
    "                track_color = cyclone_colors[i]\n",
    "            else:\n",
    "                track_color = \"black\"  # Default to black for better visibility\n",
    "\n",
    "            storm = None\n",
    "\n",
    "            # Load storm based on data source\n",
    "            if data_source == \"api\":\n",
    "                # Find storm by name and year\n",
    "                def find_storm_by_name_year(ibt, name, year):\n",
    "                    name = name.upper().strip()\n",
    "                    for storm in ibt.storms:\n",
    "                        if (\n",
    "                            hasattr(storm, \"name\")\n",
    "                            and hasattr(storm, \"season\")\n",
    "                            and storm.name.upper().strip() == name\n",
    "                            and storm.season == year\n",
    "                        ):\n",
    "                            return storm\n",
    "                    return None\n",
    "\n",
    "                if isinstance(cyclone_info, str):\n",
    "                    try:\n",
    "                        storm = ibt.get_storm_from_atcfid(cyclone_info)\n",
    "                    except:\n",
    "                        for s in ibt.storms:\n",
    "                            if hasattr(s, \"ID\") and s.ID == cyclone_info:\n",
    "                                storm = s\n",
    "                                break\n",
    "                elif len(cyclone_info) == 2:\n",
    "                    name, year = cyclone_info\n",
    "                    storm = find_storm_by_name_year(ibt, name, year)\n",
    "                else:\n",
    "                    name, year, basin = cyclone_info\n",
    "                    storm = find_storm_by_name_year(ibt, name, year)\n",
    "                    if not storm:\n",
    "                        try:\n",
    "                            storm = ibt.get_storm_from_name(\n",
    "                                name=name, season=year, basin=basin\n",
    "                            )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            elif data_source == \"csv\":\n",
    "                if isinstance(cyclone_info, (tuple, list)) and len(cyclone_info) >= 2:\n",
    "                    name, year = cyclone_info[0], cyclone_info[1]\n",
    "                    storm = load_cyclone_from_csv(\n",
    "                        csv_file_path, name, year, filter_dates\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Warning: For CSV data source, cyclone info must be (name, year). Got: {}\".format(\n",
    "                            cyclone_info\n",
    "                        )\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            if storm is None:\n",
    "                print(f\"Warning: Could not find cyclone data for {cyclone_info}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Handle data access for both API and CSV objects\n",
    "                if hasattr(storm, \"lats\"):\n",
    "                    lats_track = storm.lats\n",
    "                elif hasattr(storm, \"lat\"):\n",
    "                    lats_track = storm.lat\n",
    "                else:\n",
    "                    print(\"Warning: Storm object has no latitude data\")\n",
    "                    continue\n",
    "\n",
    "                if hasattr(storm, \"lons\"):\n",
    "                    lons_track = storm.lons\n",
    "                elif hasattr(storm, \"lon\"):\n",
    "                    lons_track = storm.lon\n",
    "                else:\n",
    "                    print(\"Warning: Storm object has no longitude data\")\n",
    "                    continue\n",
    "\n",
    "                # Handle longitudes > 180\n",
    "                lons_track = np.array(lons_track)\n",
    "                lons_track = np.where(lons_track > 180, lons_track - 360, lons_track)\n",
    "\n",
    "                # Plot track line with better visibility\n",
    "                ax.plot(\n",
    "                    lons_track,\n",
    "                    lats_track,\n",
    "                    color=track_color,\n",
    "                    linewidth=2.0,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    zorder=20,  # Above precipitation\n",
    "                    path_effects=[\n",
    "                        pe.withStroke(linewidth=3.5, foreground=\"white\")\n",
    "                    ],  # White outline\n",
    "                )\n",
    "\n",
    "                # Add markers at track points (similar to the reference image)\n",
    "                # Mark start and end points\n",
    "                ax.plot(\n",
    "                    lons_track[0],\n",
    "                    lats_track[0],\n",
    "                    \"o\",\n",
    "                    color=track_color,\n",
    "                    markersize=7,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    zorder=25,\n",
    "                    path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")],\n",
    "                )\n",
    "\n",
    "                # Mark end point\n",
    "                ax.plot(\n",
    "                    lons_track[-1],\n",
    "                    lats_track[-1],\n",
    "                    \"o\",\n",
    "                    color=track_color,\n",
    "                    markersize=7,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    zorder=25,\n",
    "                    path_effects=[pe.withStroke(linewidth=1.5, foreground=\"white\")],\n",
    "                )\n",
    "\n",
    "                # Add TC name label - find a point within bbox if provided\n",
    "                if hasattr(storm, \"name\"):\n",
    "                    label_idx = 0  # Default to first point\n",
    "\n",
    "                    # Try to find a point within the bbox for better label placement\n",
    "                    if bbox is not None:\n",
    "                        min_lat, max_lat, min_lon, max_lon = bbox\n",
    "                        # Find track points within the bbox\n",
    "                        within_bbox_indices = []\n",
    "                        for idx, (lat, lon) in enumerate(zip(lats_track, lons_track)):\n",
    "                            if min_lat <= lat <= max_lat and min_lon <= lon <= max_lon:\n",
    "                                within_bbox_indices.append(idx)\n",
    "\n",
    "                        if within_bbox_indices:\n",
    "                            # Use the middle point within bbox for label\n",
    "                            label_idx = within_bbox_indices[\n",
    "                                len(within_bbox_indices) // 2\n",
    "                            ]\n",
    "                            print(\n",
    "                                f\"Placed TC {storm.name} label at track point {label_idx} (within bbox)\"\n",
    "                            )\n",
    "                        else:\n",
    "                            # If no points in bbox, use the closest point to bbox center\n",
    "                            bbox_center_lat = (min_lat + max_lat) / 2\n",
    "                            bbox_center_lon = (min_lon + max_lon) / 2\n",
    "                            distances = [\n",
    "                                (lat - bbox_center_lat) ** 2\n",
    "                                + (lon - bbox_center_lon) ** 2\n",
    "                                for lat, lon in zip(lats_track, lons_track)\n",
    "                            ]\n",
    "                            label_idx = np.argmin(distances)\n",
    "                            print(\n",
    "                                f\"Placed TC {storm.name} label at closest point to bbox center (index {label_idx})\"\n",
    "                            )\n",
    "\n",
    "                    # Position offset based on reference image\n",
    "                    x_offset = 0  # No horizontal offset\n",
    "                    y_offset = -1.0  # Below the track point\n",
    "\n",
    "                    ax.text(\n",
    "                        lons_track[label_idx] + x_offset,\n",
    "                        lats_track[label_idx] + y_offset,\n",
    "                        f\"TC {storm.name}\",  # Just the name, not uppercase\n",
    "                        color=\"black\",\n",
    "                        fontsize=10,\n",
    "                        fontweight=\"bold\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")],\n",
    "                        transform=ccrs.PlateCarree(),\n",
    "                        zorder=90,\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error processing track data for {getattr(storm, 'name', 'Unknown Storm')}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding cyclone tracks: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 2: Using CSV data source with date filtering (if you have the local csv file for ibtracs)\n",
    "    print(\"Example 2: Cyclone Freddy using CSV data with date filtering\")\n",
    "    bbox_idai = (-27, -9, 29, 46)  # Bounding box for Freddy\n",
    "    countries_idai = [\"Mozambique\", \"Malawi\", \"Madagascar\", \"Zimbabwe\"]\n",
    "    country_colors_idai = [\"darkblue\", \"lightgreen\", \"purple\", \"orange\"]\n",
    "    track_colors_idai = [\"black\"]\n",
    "\n",
    "    # Path to your precipitation file for Freddy\n",
    "    nc_file_idai = (\n",
    "        r\"p:/wflow_global/hydromt/meteo/era5/tp/era5_tp_2019_hourly.nc\"\n",
    "    )\n",
    "\n",
    "    # Path to your CSV file\n",
    "    csv_path = r\"P:\\11210471-001-compass\\01_Data\\IBTrACS\\ibtracs.SI.list.v04r01.csv\"\n",
    "\n",
    "    # Define date range for filtering\n",
    "    ts_start = start_date\n",
    "    ts_end = end_date\n",
    "\n",
    "    fig2, ax2 = plot_cyclone_precipitation(\n",
    "        nc_file=nc_file_idai,\n",
    "        bbox=(-27, -9, 29, 46),\n",
    "        cyclone_list=[(\"IDAI\", 2019)],  # Use CSV format (name, year)\n",
    "        highlight_countries=countries_idai,\n",
    "        highlight_colors=country_colors_idai,\n",
    "        cyclone_colors=track_colors_idai,\n",
    "        title=None,  # Will use automatic TC name labeling\n",
    "        output_file=r\"P:\\11210471-001-compass\\code\\preprocessing_figures\\idai_precip_csv_filtered_doris.png\",\n",
    "        min_precipitation=50.0,\n",
    "        max_precipitation=700.0,\n",
    "        include_track=True,\n",
    "        country_label_size=0,\n",
    "        dpi=300,\n",
    "        data_source=\"csv\",  # Use CSV data source\n",
    "        csv_file_path=csv_path,  # Path to CSV file\n",
    "        filter_dates=(ts_start, ts_end),  # Filter track to specific date range\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 2: Using CSV data source with date filtering (if you have the local csv file for ibtracs)\n",
    "    print(\"Example 2: Cyclone Freddy using CSV data with date filtering\")\n",
    "    bbox_idai = bbox_big  # Bounding box for Freddy\n",
    "    countries_idai = [\"Mozambique\", \"Malawi\", \"Madagascar\", \"Zimbabwe\"]\n",
    "    country_colors_idai = [\"darkblue\", \"lightgreen\", \"purple\", \"orange\"]\n",
    "    track_colors_idai = [\"black\"]\n",
    "\n",
    "    # Path to your precipitation file for Freddy\n",
    "    nc_file_idai = (\n",
    "        r\"p:\\11210471-001-compass\\01_Data\\ERA5\\Idai\\time_corrected\\era5_tp_long.nc\"\n",
    "    )\n",
    "\n",
    "    # Path to your CSV file\n",
    "    csv_path = r\"P:\\11210471-001-compass\\01_Data\\IBTrACS\\ibtracs.SI.list.v04r01.csv\"\n",
    "\n",
    "    # Define date range for filtering\n",
    "    ts_start = start_date\n",
    "    ts_end = end_date\n",
    "\n",
    "    fig2, ax2 = plot_cyclone_precipitation(\n",
    "        nc_file=nc_file_idai,\n",
    "        bbox=(-27, -9, 29, 46),\n",
    "        cyclone_list=[(\"IDAI\", 2019)],  # Use CSV format (name, year)\n",
    "        highlight_countries=countries_idai,\n",
    "        highlight_colors=country_colors_idai,\n",
    "        cyclone_colors=track_colors_idai,\n",
    "        title=None,  # Will use automatic TC name labeling\n",
    "        output_file=r\"P:\\11210471-001-compass\\code\\preprocessing_figures\\idai_precip_csv_filtered_doris.png\",\n",
    "        min_precipitation=50.0,\n",
    "        max_precipitation=700.0,\n",
    "        include_track=True,\n",
    "        country_label_size=0,\n",
    "        dpi=300,\n",
    "        data_source=\"csv\",  # Use CSV data source\n",
    "        csv_file_path=csv_path,  # Path to CSV file\n",
    "        filter_dates=(ts_start, ts_end),  # Filter track to specific date range\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-27, -9, 29, 46)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydromt-sfincs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
